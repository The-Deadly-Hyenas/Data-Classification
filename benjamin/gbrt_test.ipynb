{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Gradient Boosted Regression Trees\n",
    "---\n",
    "\n",
    "#### Data Classification Task - Model Training and Testing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports\n",
    "---"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "from imblearn.combine import SMOTEENN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Preprocessing\n",
    "---\n",
    "Where the ✨ magic ✨ happens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "data = pd.read_pickle('../training_dataset_task3/task_3_training_e8da4715deef7d56_f8b7378_pandas.pkl')\n",
    "df = data.loc[:,'pianist_id':'arousal']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# drop non-required features\n",
    "X = df.drop(['quadrant', 'valence', 'arousal'], axis=1)\n",
    "y = df['quadrant']\n",
    "\n",
    "# normalize data\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "X = pd.DataFrame(X_std, columns=X.columns)\n",
    "X = X.loc[:, \"essentia_dissonance_mean\":\"mirtoolbox_roughness_pct_90\"]\n",
    "\n",
    "# shuffle data\n",
    "X, y = shuffle(X, y , random_state=13)\n",
    "\n",
    "# smote enn\n",
    "smote_enn = SMOTEENN(random_state=0)\n",
    "X_resampled, y_resampled = smote_enn.fit_resample(X, y)\n",
    "\n",
    "# split data into training-/test-set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "     essentia_dissonance_mean  essentia_dissonance_stdev  \\\n78                   0.652131                  -0.395405   \n29                   0.270427                  -0.674596   \n280                 -1.648462                  -0.008504   \n507                  1.295009                  -0.834411   \n652                  0.677167                   0.071451   \n..                        ...                        ...   \n106                  0.582137                  -0.013801   \n270                 -0.221779                   0.842557   \n860                  0.174741                   0.626478   \n435                  1.053793                  -0.742868   \n102                  0.127936                  -0.858591   \n\n     essentia_dynamic_complexity  essentia_loudness  essentia_onset_rate  \\\n78                     -0.947066           0.617313             1.135133   \n29                     -0.673356           0.982513            -0.309247   \n280                    -0.808687          -0.818066            -0.523434   \n507                    -0.743406           2.526212             1.040166   \n652                    -0.139664           0.600671             0.065922   \n..                           ...                ...                  ...   \n106                    -0.213219          -0.488757             0.239327   \n270                     0.235892           0.081516            -1.041193   \n860                     3.544625          -0.313134             0.678186   \n435                    -0.749657           1.549309            -0.089818   \n102                    -0.495799           0.865913             0.019897   \n\n     essentia_pitch_salience_mean  essentia_pitch_salience_stdev  \\\n78                       0.411921                      -1.141278   \n29                       0.018125                      -0.097964   \n280                     -0.284987                      -1.463189   \n507                      0.884361                      -1.278342   \n652                      0.265566                      -0.228369   \n..                            ...                            ...   \n106                      0.004035                       0.506741   \n270                     -0.252875                       0.526253   \n860                      0.792448                       0.523175   \n435                      0.589554                      -0.752958   \n102                      0.129408                       0.135678   \n\n     essentia_spectral_centroid_mean  essentia_spectral_centroid_stdev  \\\n78                         -0.052817                          0.969386   \n29                          0.246241                         -0.265885   \n280                        -0.800746                          0.198840   \n507                         0.355783                         -0.348396   \n652                        -0.127591                         -0.278460   \n..                               ...                               ...   \n106                        -0.158201                         -0.486893   \n270                        -0.185498                         -0.216726   \n860                        -0.320751                         -0.541040   \n435                         0.005411                         -0.302675   \n102                        -0.330521                         -0.593080   \n\n     essentia_spectral_complexity_mean  ...  mirtoolbox_novelty_std  \\\n78                           -0.611714  ...               -0.283257   \n29                           -1.205762  ...               -0.986785   \n280                           0.853732  ...                0.891188   \n507                           0.482245  ...                0.482468   \n652                          -0.659449  ...               -1.356160   \n..                                 ...  ...                     ...   \n106                           0.544330  ...               -0.211979   \n270                          -0.920352  ...                0.570368   \n860                           0.401094  ...                0.779267   \n435                           0.669336  ...               -1.062230   \n102                          -0.736988  ...               -0.025846   \n\n     mirtoolbox_novelty_pct_10  mirtoolbox_novelty_pct_50  \\\n78                   -0.132523                   0.128592   \n29                   -0.132523                  -1.021422   \n280                  -0.132523                  -0.331523   \n507                  -0.132523                   0.815806   \n652                  -0.132523                  -0.643391   \n..                         ...                        ...   \n106                  -0.132523                   0.027178   \n270                  -0.132523                  -0.527514   \n860                  -0.132523                  -0.025593   \n435                  -0.132523                  -0.945741   \n102                  -0.132523                   0.059575   \n\n     mirtoolbox_novelty_pct_90  mirtoolbox_pulseclarity  \\\n78                   -0.066988                 0.500024   \n29                   -1.318619                -1.121356   \n280                   0.588847                -0.717868   \n507                   0.587096                 0.252162   \n652                  -1.019313                 2.646946   \n..                         ...                      ...   \n106                   0.212757                 0.326150   \n270                   0.306412                -0.070935   \n860                   0.591658                -0.585584   \n435                  -0.586691                -0.464582   \n102                  -0.248437                -0.552279   \n\n     mirtoolbox_roughness_mean  mirtoolbox_roughness_std  \\\n78                    0.502911                 -0.520811   \n29                   -0.334504                 -0.000192   \n280                   0.145573                  0.007299   \n507                   1.116314                 -0.119981   \n652                   0.215632                  0.050398   \n..                         ...                       ...   \n106                   0.257034                  1.770699   \n270                  -0.806342                 -0.918741   \n860                  -0.164251                  2.033246   \n435                   1.783589                 -0.631891   \n102                  -0.011280                 -0.188912   \n\n     mirtoolbox_roughness_pct_10  mirtoolbox_roughness_pct_50  \\\n78                      0.824101                     0.485116   \n29                     -0.238357                    -0.422630   \n280                     0.115105                     0.142528   \n507                     1.321130                     1.120894   \n652                     0.207223                     0.225942   \n..                           ...                          ...   \n106                    -0.064601                     0.033253   \n270                    -0.578591                    -0.775367   \n860                    -0.666144                    -0.470561   \n435                     2.134855                     1.829304   \n102                     0.113730                    -0.037762   \n\n     mirtoolbox_roughness_pct_90  \n78                      0.224715  \n29                     -0.189874  \n280                     0.045589  \n507                     0.876903  \n652                     0.176425  \n..                           ...  \n106                     0.781597  \n270                    -0.962225  \n860                     0.869274  \n435                     1.247930  \n102                    -0.122285  \n\n[803 rows x 169 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>essentia_dissonance_mean</th>\n      <th>essentia_dissonance_stdev</th>\n      <th>essentia_dynamic_complexity</th>\n      <th>essentia_loudness</th>\n      <th>essentia_onset_rate</th>\n      <th>essentia_pitch_salience_mean</th>\n      <th>essentia_pitch_salience_stdev</th>\n      <th>essentia_spectral_centroid_mean</th>\n      <th>essentia_spectral_centroid_stdev</th>\n      <th>essentia_spectral_complexity_mean</th>\n      <th>...</th>\n      <th>mirtoolbox_novelty_std</th>\n      <th>mirtoolbox_novelty_pct_10</th>\n      <th>mirtoolbox_novelty_pct_50</th>\n      <th>mirtoolbox_novelty_pct_90</th>\n      <th>mirtoolbox_pulseclarity</th>\n      <th>mirtoolbox_roughness_mean</th>\n      <th>mirtoolbox_roughness_std</th>\n      <th>mirtoolbox_roughness_pct_10</th>\n      <th>mirtoolbox_roughness_pct_50</th>\n      <th>mirtoolbox_roughness_pct_90</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>78</th>\n      <td>0.652131</td>\n      <td>-0.395405</td>\n      <td>-0.947066</td>\n      <td>0.617313</td>\n      <td>1.135133</td>\n      <td>0.411921</td>\n      <td>-1.141278</td>\n      <td>-0.052817</td>\n      <td>0.969386</td>\n      <td>-0.611714</td>\n      <td>...</td>\n      <td>-0.283257</td>\n      <td>-0.132523</td>\n      <td>0.128592</td>\n      <td>-0.066988</td>\n      <td>0.500024</td>\n      <td>0.502911</td>\n      <td>-0.520811</td>\n      <td>0.824101</td>\n      <td>0.485116</td>\n      <td>0.224715</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.270427</td>\n      <td>-0.674596</td>\n      <td>-0.673356</td>\n      <td>0.982513</td>\n      <td>-0.309247</td>\n      <td>0.018125</td>\n      <td>-0.097964</td>\n      <td>0.246241</td>\n      <td>-0.265885</td>\n      <td>-1.205762</td>\n      <td>...</td>\n      <td>-0.986785</td>\n      <td>-0.132523</td>\n      <td>-1.021422</td>\n      <td>-1.318619</td>\n      <td>-1.121356</td>\n      <td>-0.334504</td>\n      <td>-0.000192</td>\n      <td>-0.238357</td>\n      <td>-0.422630</td>\n      <td>-0.189874</td>\n    </tr>\n    <tr>\n      <th>280</th>\n      <td>-1.648462</td>\n      <td>-0.008504</td>\n      <td>-0.808687</td>\n      <td>-0.818066</td>\n      <td>-0.523434</td>\n      <td>-0.284987</td>\n      <td>-1.463189</td>\n      <td>-0.800746</td>\n      <td>0.198840</td>\n      <td>0.853732</td>\n      <td>...</td>\n      <td>0.891188</td>\n      <td>-0.132523</td>\n      <td>-0.331523</td>\n      <td>0.588847</td>\n      <td>-0.717868</td>\n      <td>0.145573</td>\n      <td>0.007299</td>\n      <td>0.115105</td>\n      <td>0.142528</td>\n      <td>0.045589</td>\n    </tr>\n    <tr>\n      <th>507</th>\n      <td>1.295009</td>\n      <td>-0.834411</td>\n      <td>-0.743406</td>\n      <td>2.526212</td>\n      <td>1.040166</td>\n      <td>0.884361</td>\n      <td>-1.278342</td>\n      <td>0.355783</td>\n      <td>-0.348396</td>\n      <td>0.482245</td>\n      <td>...</td>\n      <td>0.482468</td>\n      <td>-0.132523</td>\n      <td>0.815806</td>\n      <td>0.587096</td>\n      <td>0.252162</td>\n      <td>1.116314</td>\n      <td>-0.119981</td>\n      <td>1.321130</td>\n      <td>1.120894</td>\n      <td>0.876903</td>\n    </tr>\n    <tr>\n      <th>652</th>\n      <td>0.677167</td>\n      <td>0.071451</td>\n      <td>-0.139664</td>\n      <td>0.600671</td>\n      <td>0.065922</td>\n      <td>0.265566</td>\n      <td>-0.228369</td>\n      <td>-0.127591</td>\n      <td>-0.278460</td>\n      <td>-0.659449</td>\n      <td>...</td>\n      <td>-1.356160</td>\n      <td>-0.132523</td>\n      <td>-0.643391</td>\n      <td>-1.019313</td>\n      <td>2.646946</td>\n      <td>0.215632</td>\n      <td>0.050398</td>\n      <td>0.207223</td>\n      <td>0.225942</td>\n      <td>0.176425</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>106</th>\n      <td>0.582137</td>\n      <td>-0.013801</td>\n      <td>-0.213219</td>\n      <td>-0.488757</td>\n      <td>0.239327</td>\n      <td>0.004035</td>\n      <td>0.506741</td>\n      <td>-0.158201</td>\n      <td>-0.486893</td>\n      <td>0.544330</td>\n      <td>...</td>\n      <td>-0.211979</td>\n      <td>-0.132523</td>\n      <td>0.027178</td>\n      <td>0.212757</td>\n      <td>0.326150</td>\n      <td>0.257034</td>\n      <td>1.770699</td>\n      <td>-0.064601</td>\n      <td>0.033253</td>\n      <td>0.781597</td>\n    </tr>\n    <tr>\n      <th>270</th>\n      <td>-0.221779</td>\n      <td>0.842557</td>\n      <td>0.235892</td>\n      <td>0.081516</td>\n      <td>-1.041193</td>\n      <td>-0.252875</td>\n      <td>0.526253</td>\n      <td>-0.185498</td>\n      <td>-0.216726</td>\n      <td>-0.920352</td>\n      <td>...</td>\n      <td>0.570368</td>\n      <td>-0.132523</td>\n      <td>-0.527514</td>\n      <td>0.306412</td>\n      <td>-0.070935</td>\n      <td>-0.806342</td>\n      <td>-0.918741</td>\n      <td>-0.578591</td>\n      <td>-0.775367</td>\n      <td>-0.962225</td>\n    </tr>\n    <tr>\n      <th>860</th>\n      <td>0.174741</td>\n      <td>0.626478</td>\n      <td>3.544625</td>\n      <td>-0.313134</td>\n      <td>0.678186</td>\n      <td>0.792448</td>\n      <td>0.523175</td>\n      <td>-0.320751</td>\n      <td>-0.541040</td>\n      <td>0.401094</td>\n      <td>...</td>\n      <td>0.779267</td>\n      <td>-0.132523</td>\n      <td>-0.025593</td>\n      <td>0.591658</td>\n      <td>-0.585584</td>\n      <td>-0.164251</td>\n      <td>2.033246</td>\n      <td>-0.666144</td>\n      <td>-0.470561</td>\n      <td>0.869274</td>\n    </tr>\n    <tr>\n      <th>435</th>\n      <td>1.053793</td>\n      <td>-0.742868</td>\n      <td>-0.749657</td>\n      <td>1.549309</td>\n      <td>-0.089818</td>\n      <td>0.589554</td>\n      <td>-0.752958</td>\n      <td>0.005411</td>\n      <td>-0.302675</td>\n      <td>0.669336</td>\n      <td>...</td>\n      <td>-1.062230</td>\n      <td>-0.132523</td>\n      <td>-0.945741</td>\n      <td>-0.586691</td>\n      <td>-0.464582</td>\n      <td>1.783589</td>\n      <td>-0.631891</td>\n      <td>2.134855</td>\n      <td>1.829304</td>\n      <td>1.247930</td>\n    </tr>\n    <tr>\n      <th>102</th>\n      <td>0.127936</td>\n      <td>-0.858591</td>\n      <td>-0.495799</td>\n      <td>0.865913</td>\n      <td>0.019897</td>\n      <td>0.129408</td>\n      <td>0.135678</td>\n      <td>-0.330521</td>\n      <td>-0.593080</td>\n      <td>-0.736988</td>\n      <td>...</td>\n      <td>-0.025846</td>\n      <td>-0.132523</td>\n      <td>0.059575</td>\n      <td>-0.248437</td>\n      <td>-0.552279</td>\n      <td>-0.011280</td>\n      <td>-0.188912</td>\n      <td>0.113730</td>\n      <td>-0.037762</td>\n      <td>-0.122285</td>\n    </tr>\n  </tbody>\n</table>\n<p>803 rows × 169 columns</p>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Traing and Evaluation\n",
    "---"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate model performance\n",
    "    :param model:\n",
    "    :param X_test:\n",
    "    :param y_test:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    test_yhat = model.predict(X_test)\n",
    "    errors = abs(test_yhat - y_test)\n",
    "    # mape = 100 * np.mean(errors / y_test)\n",
    "\n",
    "    train_yhat = model.predict(X_train)\n",
    "    test_yhat = model.predict(X_test)\n",
    "\n",
    "    print('Model Performance Check:')\n",
    "    print(\"***\"*3)\n",
    "    print('Average Error: {:0.4f}'.format(np.mean(errors)))\n",
    "    print('MSE: {:0.4f}'.format(mean_squared_error(y_test, model.predict(X_test))))\n",
    "    # print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "    print(\"\\nModel Overfitting Check:\")\n",
    "    print(\"***\"*3)\n",
    "    print(\"Trainset Accuracy: {:0.4f}\".format(accuracy_score(y_train, train_yhat)))\n",
    "    print(\"Testset Accuracy: {:0.4f}\".format(accuracy_score(y_test, test_yhat)))\n",
    "\n",
    "def export_model(model, model_name: str):\n",
    "    \"\"\"\n",
    "    Exports a given classifier to a pickel file\n",
    "    :param model:\n",
    "    :param model_name:\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if model is not None:\n",
    "        try:\n",
    "            joblib.dump(gbrt_model, '{}.pkl'.format(model_name), compress=9)\n",
    "            print(\"Successfully exported classifier!\\n Location: {}\")\n",
    "        except Exception as ex:\n",
    "            raise ex\n",
    "    else:\n",
    "        raise ValueError(\"Model is none\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "GradientBoostingClassifier(max_depth=50, min_samples_leaf=2, n_estimators=200,\n                           random_state=0)",
      "text/html": "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingClassifier(max_depth=50, min_samples_leaf=2, n_estimators=200,\n                           random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(max_depth=50, min_samples_leaf=2, n_estimators=200,\n                           random_state=0)</pre></div></div></div></div></div>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient Boosted Regression Tree\n",
    "gbrt_model = GradientBoostingClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=50,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=0).fit(X_train, y_train)\n",
    "\n",
    "gbrt_model.fit(X_train,y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance Check:\n",
      "*********\n",
      "Average Error: 0.2736\n",
      "MSE: 0.5124\n",
      "\n",
      "Model Overfitting Check:\n",
      "*********\n",
      "Trainset Accuracy: 1.0000\n",
      "Testset Accuracy: 0.8209\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(gbrt_model, X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Iterative Evaluation of Tree Depth\n",
    "---"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">40, train: 1.000, test: 0.925\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [43]\u001B[0m, in \u001B[0;36m<cell line: 6>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      4\u001B[0m values \u001B[38;5;241m=\u001B[39m [i \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m40\u001B[39m, \u001B[38;5;241m51\u001B[39m)]\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m values:\n\u001B[1;32m      7\u001B[0m \t\u001B[38;5;66;03m# configure the model\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \tmodel \u001B[38;5;241m=\u001B[39m \u001B[43mGradientBoostingClassifier\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_estimators\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_depth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmin_samples_leaf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m---> 13\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m \t\u001B[38;5;66;03m# fit model on the training dataset\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \tmodel\u001B[38;5;241m.\u001B[39mfit(X_train, y_train)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/deadlyhyenas/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:668\u001B[0m, in \u001B[0;36mBaseGradientBoosting.fit\u001B[0;34m(self, X, y, sample_weight, monitor)\u001B[0m\n\u001B[1;32m    665\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_resize_state()\n\u001B[1;32m    667\u001B[0m \u001B[38;5;66;03m# fit the boosting stages\u001B[39;00m\n\u001B[0;32m--> 668\u001B[0m n_stages \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_stages\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    669\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    670\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    671\u001B[0m \u001B[43m    \u001B[49m\u001B[43mraw_predictions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    672\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    673\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_rng\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    674\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX_val\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    675\u001B[0m \u001B[43m    \u001B[49m\u001B[43my_val\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    676\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_weight_val\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    677\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbegin_at_stage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    678\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmonitor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    679\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    681\u001B[0m \u001B[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001B[39;00m\n\u001B[1;32m    682\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n_stages \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimators_\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]:\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/deadlyhyenas/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:745\u001B[0m, in \u001B[0;36mBaseGradientBoosting._fit_stages\u001B[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001B[0m\n\u001B[1;32m    738\u001B[0m     old_oob_score \u001B[38;5;241m=\u001B[39m loss_(\n\u001B[1;32m    739\u001B[0m         y[\u001B[38;5;241m~\u001B[39msample_mask],\n\u001B[1;32m    740\u001B[0m         raw_predictions[\u001B[38;5;241m~\u001B[39msample_mask],\n\u001B[1;32m    741\u001B[0m         sample_weight[\u001B[38;5;241m~\u001B[39msample_mask],\n\u001B[1;32m    742\u001B[0m     )\n\u001B[1;32m    744\u001B[0m \u001B[38;5;66;03m# fit next stage of trees\u001B[39;00m\n\u001B[0;32m--> 745\u001B[0m raw_predictions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_stage\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    746\u001B[0m \u001B[43m    \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    747\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    748\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    749\u001B[0m \u001B[43m    \u001B[49m\u001B[43mraw_predictions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    750\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    751\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    752\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    753\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX_csc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    754\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX_csr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    755\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    757\u001B[0m \u001B[38;5;66;03m# track deviance (= loss)\u001B[39;00m\n\u001B[1;32m    758\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m do_oob:\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/deadlyhyenas/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:247\u001B[0m, in \u001B[0;36mBaseGradientBoosting._fit_stage\u001B[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001B[0m\n\u001B[1;32m    244\u001B[0m     sample_weight \u001B[38;5;241m=\u001B[39m sample_weight \u001B[38;5;241m*\u001B[39m sample_mask\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39mfloat64)\n\u001B[1;32m    246\u001B[0m X \u001B[38;5;241m=\u001B[39m X_csr \u001B[38;5;28;01mif\u001B[39;00m X_csr \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m X\n\u001B[0;32m--> 247\u001B[0m \u001B[43mtree\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresidual\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheck_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    249\u001B[0m \u001B[38;5;66;03m# update tree leaves\u001B[39;00m\n\u001B[1;32m    250\u001B[0m loss\u001B[38;5;241m.\u001B[39mupdate_terminal_regions(\n\u001B[1;32m    251\u001B[0m     tree\u001B[38;5;241m.\u001B[39mtree_,\n\u001B[1;32m    252\u001B[0m     X,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    259\u001B[0m     k\u001B[38;5;241m=\u001B[39mk,\n\u001B[1;32m    260\u001B[0m )\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/deadlyhyenas/lib/python3.10/site-packages/sklearn/tree/_classes.py:1342\u001B[0m, in \u001B[0;36mDecisionTreeRegressor.fit\u001B[0;34m(self, X, y, sample_weight, check_input)\u001B[0m\n\u001B[1;32m   1313\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y, sample_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, check_input\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[1;32m   1314\u001B[0m     \u001B[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001B[39;00m\n\u001B[1;32m   1315\u001B[0m \n\u001B[1;32m   1316\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1339\u001B[0m \u001B[38;5;124;03m        Fitted estimator.\u001B[39;00m\n\u001B[1;32m   1340\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1342\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1343\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1344\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1345\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1346\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcheck_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcheck_input\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1347\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1348\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/deadlyhyenas/lib/python3.10/site-packages/sklearn/tree/_classes.py:458\u001B[0m, in \u001B[0;36mBaseDecisionTree.fit\u001B[0;34m(self, X, y, sample_weight, check_input)\u001B[0m\n\u001B[1;32m    447\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    448\u001B[0m     builder \u001B[38;5;241m=\u001B[39m BestFirstTreeBuilder(\n\u001B[1;32m    449\u001B[0m         splitter,\n\u001B[1;32m    450\u001B[0m         min_samples_split,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    455\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmin_impurity_decrease,\n\u001B[1;32m    456\u001B[0m     )\n\u001B[0;32m--> 458\u001B[0m \u001B[43mbuilder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuild\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtree_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    460\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_outputs_ \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m is_classifier(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    461\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_classes_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_classes_[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# # define lists to collect scores\n",
    "# train_scores, test_scores = list(), list()\n",
    "# # define the tree depths to evaluate\n",
    "# values = [i for i in range(40, 51)]\n",
    "#\n",
    "# for i in values:\n",
    "# \t# configure the model\n",
    "# \tmodel = GradientBoostingClassifier(\n",
    "#         n_estimators=100,\n",
    "#         learning_rate=0.1,\n",
    "#         max_depth=i,\n",
    "#         min_samples_leaf=2,\n",
    "#         random_state=0).fit(X_train, y_train)\n",
    "#\n",
    "# \t# fit model on the training dataset\n",
    "# \tmodel.fit(X_train, y_train)\n",
    "# \t# evaluate on the train dataset\n",
    "# \ttrain_yhat = model.predict(X_train)\n",
    "# \ttrain_acc = accuracy_score(y_train, train_yhat)\n",
    "# \ttrain_scores.append(train_acc)\n",
    "# \t# evaluate on the test dataset\n",
    "# \ttest_yhat = model.predict(X_test)\n",
    "# \ttest_acc = accuracy_score(y_test, test_yhat)\n",
    "# \ttest_scores.append(test_acc)\n",
    "# \t# summarize progress\n",
    "# \tprint('>%d, train: %.3f, test: %.3f' % (i, train_acc, test_acc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # plot of train and test scores vs tree depth\n",
    "# plt.plot(values, train_scores, '-o', label='Train')\n",
    "# plt.plot(values, test_scores, '-o', label='Test')\n",
    "# plt.title(\"Gradient Boosted Regression Tree\")\n",
    "# plt.xlabel(\"Tree Depth\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}